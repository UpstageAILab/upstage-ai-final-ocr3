{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import pyclipper\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "from hydra import initialize, compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from runners import predict\n",
    "from ocr.datasets.base import OCRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_304914/2759098359.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"configs\"):\n",
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at ./outputs/ocr_training_2024-04-25_11-00-48/checkpoints/epoch=7-step=18552.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at ./outputs/ocr_training_2024-04-25_11-00-48/checkpoints/epoch=7-step=18552.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf34fb0f5ed24a098b75246a3740f4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with initialize(config_path=\"configs\"):\n",
    "    cfg = compose(\"predict.yaml\")\n",
    "\n",
    "model_module, data_module, temp = predict.predict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         ...,\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "\n",
       "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         ...,\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "\n",
       "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         ...,\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.dataset['predict'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         ...,\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "\n",
       "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         ...,\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "\n",
       "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         ...,\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_module.dataset['predict'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 640, 640])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.dataset['predict'][0]['image'].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_module.dataset['predict'][0]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 추론 결과 후처리를 위한 Class\n",
    "\n",
    "class DBPostProcessor:\n",
    "    def __init__(self, thresh=0.3, box_thresh=0.4, max_candidates=300, use_polygon=False):\n",
    "        self.min_size = 3                       # 텍스트 검출 최소 사이즈\n",
    "        self.thresh = thresh                    # 모델의 출력 결과 Segmentation map을 Binarization 하기 위한 Threshold\n",
    "        self.box_thresh = box_thresh            # 텍스트 영역 판정을 위한 Threshold\n",
    "        self.max_candidates = max_candidates    # 최대 텍스트 검출 후보 영역의 갯수\n",
    "        self.use_polygon = use_polygon          # Polygon 검출을 할지 Quad 검출을 할지에 대한 Type flag\n",
    "\n",
    "    # Segmentation map 결과로부터 텍스트 영역의 좌표를 반환하는 메소드\n",
    "    def represent(self, batch, _pred):\n",
    "        \"\"\"\n",
    "        batch: a dict produced by dataloaders.\n",
    "            images: tensor of shape (N, C, H, W).\n",
    "            polygons: tensor of shape (N, K, 4, 2), the polygons of objective regions.\n",
    "            ignore_tags: tensor of shape (N, K), indicates whether a region is ignorable or not.\n",
    "            shape: the original shape of images.\n",
    "            inverse_matrix: Warp Perspective Matrix, with shape (3, 3) as NDArray[float32]\n",
    "            filename: the original filenames of images.\n",
    "        pred:\n",
    "            prob_maps: text region segmentation map, with shape (N, 1, H, W)\n",
    "        \"\"\"\n",
    "        # assert 'images' in batch is not None, \"images is required in batch\"\n",
    "        images = batch['image']\n",
    "\n",
    "        # Use prob_maps if pred is a dict\n",
    "        if isinstance(_pred, dict):\n",
    "            assert 'prob_maps' in _pred is not None, \"prob_maps is required in _pred\"\n",
    "            pred = _pred['prob_maps']\n",
    "        else:\n",
    "            pred = _pred\n",
    "\n",
    "        assert 'inverse_matrix' in batch is not None, \"inverse_matrix is required in batch\"\n",
    "        inverse_matrix = batch['inverse_matrix']\n",
    "\n",
    "        # Probability map을 이진화\n",
    "        segmentation = self.binarize(pred)\n",
    "\n",
    "        boxes_batch = []\n",
    "        scores_batch = []\n",
    "        for batch_index in range(images.size(0)):\n",
    "            if self.use_polygon:\n",
    "                # Get polygons from segmentation\n",
    "                boxes, scores = self.polygons_from_bitmap(\n",
    "                                        pred[batch_index],\n",
    "                                        segmentation[batch_index],\n",
    "                                        inverse_matrix=inverse_matrix[batch_index])\n",
    "            else:\n",
    "                # Get boxes from segmentation\n",
    "                boxes, scores = self.boxes_from_bitmap(\n",
    "                                        pred[batch_index],\n",
    "                                        segmentation[batch_index],\n",
    "                                        inverse_matrix=inverse_matrix[batch_index])\n",
    "            # Append to batch\n",
    "            boxes_batch.append(boxes)\n",
    "            scores_batch.append(scores)\n",
    "\n",
    "        return boxes_batch, scores_batch\n",
    "\n",
    "    @staticmethod\n",
    "    # Inverse matrix를 이용하여 원본 이미지에 대한 좌표로 변환 하는 메소드\n",
    "    def __transform_coordinates(coords, matrix):\n",
    "        \"\"\"\n",
    "        Transform coordinates according to the warp matrix\n",
    "\n",
    "        coords: (N, 2) as NDArray[float32]\n",
    "        matrix: (3, 3) as NDArray[float32]\n",
    "        return: (N, 2) as NDArray[float32]\n",
    "        \"\"\"\n",
    "        coords = np.array(coords)\n",
    "        coords = np.dot(matrix, np.vstack([coords.T, np.ones(coords.shape[0])]))\n",
    "        coords /= coords[2, :]\n",
    "        return coords.T[:, :2]\n",
    "\n",
    "    # Binarize the prediction\n",
    "    def binarize(self, pred):\n",
    "        # 후처리 과정에서 단일 Threshold를 사용하여 이진화\n",
    "        return pred > torch.Tensor([self.thresh]).to(device=pred.device)\n",
    "\n",
    "    # 추론 결과 영역에서 텍스트 영역을 검출하여 반환하는 메소드 (Polygon으로 추출할때)\n",
    "    def polygons_from_bitmap(self, pred, _bitmap,\n",
    "                             inverse_matrix=None):\n",
    "        \"\"\"\n",
    "        Extracts polygons and their scores from a bitmap image.\n",
    "\n",
    "        _bitmap: single map with shape (1, H, W),\n",
    "            whose values are binarized as {0, 1}\n",
    "        \"\"\"\n",
    "\n",
    "        assert _bitmap.size(0) == 1\n",
    "        bitmap = _bitmap.cpu().numpy()[0]  # The first channel\n",
    "        pred = pred.cpu().detach().numpy()[0]\n",
    "\n",
    "        boxes = []\n",
    "        scores = []\n",
    "\n",
    "        # 컨투어 영역 찾기\n",
    "        # Find contours from the binarized map\n",
    "        # contours: a list of contours\n",
    "        # https://docs.opencv.org/4.9.0/d4/d73/tutorial_py_contours_begin.html\n",
    "        contours, _ = cv2.findContours(\n",
    "            (bitmap * 255).astype(np.uint8),\n",
    "            cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # 최대 텍스트 검출 후보 영역의 갯수만큼 컨투어 영역이 텍스트 영역인지 판단\n",
    "        # Get the top N contours\n",
    "        for contour in contours[:self.max_candidates]:\n",
    "            # 복잡한 컨투어 픽셀을 근사화하여 영역 다듬기\n",
    "            # Approximate the contour with Douglas-Peucker algorithm\n",
    "            # https://docs.opencv.org/4.9.0/dc/dcf/tutorial_js_contour_features.html\n",
    "            epsilon = 0.002 * cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "            points = approx.reshape((-1, 2))\n",
    "            # 최소 포인트가 4개 미만인 경우 무시\n",
    "            if points.shape[0] < 4:\n",
    "                continue\n",
    "\n",
    "            # 컨투어 영역에서 텍스트가 차지하는 영역에 대한 점수 계산\n",
    "            # Get the score of the box\n",
    "            score = self.box_score_fast(pred, points.reshape(-1, 2))\n",
    "            if self.box_thresh > score:\n",
    "                continue\n",
    "\n",
    "            # Shrink되어있는 추론 결과를 다시 Dilate하는 과정\n",
    "            # Unclip the box\n",
    "            if points.shape[0] > 2:\n",
    "                box = self.unclip(points, unclip_ratio=2.0)\n",
    "                if box is None:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # 주어진 영역의 크기를 구하고 최소 사이즈 미만의 경우 무시\n",
    "            # Get the mini box\n",
    "            box = box.reshape(-1, 2)\n",
    "            _, sside = self.get_mini_boxes(box.reshape((-1, 1, 2)))\n",
    "            if sside < self.min_size + 2:\n",
    "                continue\n",
    "\n",
    "            # Inverse Matrix를 이용하여 원본 이미지의 위치로 좌표 변환\n",
    "            # Transform the coordinates\n",
    "            box = self.__transform_coordinates(box, inverse_matrix)\n",
    "\n",
    "            # Append to the list\n",
    "            boxes.append(np.round(box).astype(np.int16).tolist())\n",
    "            scores.append(score)\n",
    "\n",
    "        return boxes, scores\n",
    "\n",
    "    # 추론 결과 영역에서 텍스트 영역을 검출하여 반환하는 메소드 (QuadBox로 추출할때)\n",
    "    # 텍스트 영역을 찾는 과정을 제외하고 Polygon과 동일\n",
    "    def boxes_from_bitmap(self, pred, _bitmap,\n",
    "                          inverse_matrix=None):\n",
    "        \"\"\"\n",
    "        Extracts bounding boxes and their scores from a bitmap image.\n",
    "\n",
    "        _bitmap: single map with shape (1, H, W),\n",
    "            whose values are binarized as {0, 1}\n",
    "        \"\"\"\n",
    "\n",
    "        assert _bitmap.size(0) == 1\n",
    "        bitmap = _bitmap.cpu().numpy()[0]  # The first channel\n",
    "        pred = pred.cpu().detach().numpy()[0]\n",
    "\n",
    "        boxes = []\n",
    "        scores = []\n",
    "\n",
    "        # 컨투어 영역 찾기\n",
    "        # Find contours from the binarized map\n",
    "        # contours: a list of contours\n",
    "        # https://docs.opencv.org/4.9.0/d4/d73/tutorial_py_contours_begin.html\n",
    "        contours, _ = cv2.findContours(\n",
    "            (bitmap * 255).astype(np.uint8),\n",
    "            cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        num_contours = min(len(contours), self.max_candidates)\n",
    "\n",
    "        # 최대 텍스트 검출 후보 영역의 갯수만큼 컨투어 영역이 텍스트 영역인지 판단\n",
    "        # Get the top N contours\n",
    "        for index in range(num_contours):\n",
    "            # 컨투어 영역의 좌표와 크기를 구하고 최소 사이즈 이하의 경우 무시\n",
    "            # Get the mini box\n",
    "            contour = contours[index]\n",
    "            points, sside = self.get_mini_boxes(contour)\n",
    "            if sside < self.min_size:\n",
    "                continue\n",
    "\n",
    "            # 컨투어 영역에서 텍스트가 차지하는 영역에 대한 점수 계산\n",
    "            # Get the score of the box\n",
    "            points = np.array(points)\n",
    "            score = self.box_score_fast(pred, points.reshape(-1, 2))\n",
    "            if self.box_thresh > score:\n",
    "                continue\n",
    "\n",
    "            # Shrink되어있는 추론 결과를 다시 Dilate하는 과정\n",
    "            # Unclip the box\n",
    "            box = self.unclip(points).reshape(-1, 1, 2)\n",
    "            box, sside = self.get_mini_boxes(box)\n",
    "            if sside < self.min_size + 2:\n",
    "                continue\n",
    "            box = np.array(box)\n",
    "\n",
    "            # Inverse Matrix를 이용하여 원본 이미지의 위치로 좌표 변환\n",
    "            # Transform the coordinates\n",
    "            box = self.__transform_coordinates(box, inverse_matrix)\n",
    "\n",
    "            # Append to the list\n",
    "            boxes.append(np.round(box).astype(np.int16).tolist())\n",
    "            scores.append(score)\n",
    "\n",
    "        return boxes, scores\n",
    "\n",
    "    def unclip(self, box, unclip_ratio=1.5):\n",
    "        \"\"\"\n",
    "        Expands the given box by a specified ratio.\n",
    "\n",
    "        box: a list of points of shape (N, 2)\n",
    "        unclip_ratio: the ratio of unclipping the box\n",
    "        return: a list of points of shape (N, 2)\n",
    "        \"\"\"\n",
    "\n",
    "        # transform the box to polygon\n",
    "        poly = Polygon(box)\n",
    "        if poly.area == 0 or poly.length == 0:\n",
    "            return None\n",
    "\n",
    "        # get the expanded polygon\n",
    "        # unclip_ratio의 비율로 확대된 Distance를 구하고 영역을 확장함\n",
    "        distance = poly.area * unclip_ratio / poly.length\n",
    "        offset = pyclipper.PyclipperOffset()\n",
    "        offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n",
    "        expanded = np.array(offset.Execute(distance)[0])\n",
    "\n",
    "        return expanded\n",
    "\n",
    "    def get_mini_boxes(self, contour):\n",
    "        \"\"\"\n",
    "        Converts a contour into its minimum area bounding box.\n",
    "\n",
    "        contour: a list of points of shape (N, 1, 2)\n",
    "        return: a list of points of shape (N, 2)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the bounding box\n",
    "        # https://docs.opencv.org/4.9.0/de/d62/tutorial_bounding_rotated_ellipses.html\n",
    "        bounding_box = cv2.minAreaRect(contour)\n",
    "        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n",
    "\n",
    "        index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n",
    "        if points[1][1] > points[0][1]:\n",
    "            index_1 = 0\n",
    "            index_4 = 1\n",
    "        else:\n",
    "            index_1 = 1\n",
    "            index_4 = 0\n",
    "        if points[3][1] > points[2][1]:\n",
    "            index_2 = 2\n",
    "            index_3 = 3\n",
    "        else:\n",
    "            index_2 = 3\n",
    "            index_3 = 2\n",
    "\n",
    "        box = [points[index_1], points[index_2],\n",
    "               points[index_3], points[index_4]]\n",
    "        return box, min(bounding_box[1])\n",
    "\n",
    "    def box_score_fast(self, bitmap, _box):\n",
    "        \"\"\"\n",
    "        Calculates a score for a box in a bitmap.\n",
    "        The score is the percentage of the box area that overlaps with\n",
    "        the highlighted areas (marked as 1) in the bitmap.\n",
    "\n",
    "        bitmap: a single map with shape (H, W), whose values are binarized as {0, 1}\n",
    "        _box: a list of points of shape (N, 2)\n",
    "        return: a score of the box as float32\n",
    "        \"\"\"\n",
    "\n",
    "        h, w = bitmap.shape[:2]\n",
    "        box = _box.copy()\n",
    "        xmin = np.clip(np.floor(box[:, 0].min()).astype(np.int32), 0, w - 1)\n",
    "        xmax = np.clip(np.ceil(box[:, 0].max()).astype(np.int32), 0, w - 1)\n",
    "        ymin = np.clip(np.floor(box[:, 1].min()).astype(np.int32), 0, h - 1)\n",
    "        ymax = np.clip(np.ceil(box[:, 1].max()).astype(np.int32), 0, h - 1)\n",
    "\n",
    "        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n",
    "        box[:, 0] = box[:, 0] - xmin\n",
    "        box[:, 1] = box[:, 1] - ymin\n",
    "\n",
    "        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(np.int32), 1)\n",
    "\n",
    "        return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processor = DBPostProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 640])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]['prob_maps'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 640])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]['prob_maps'].squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(temp)):\n",
    "    temp[i]['prob_maps'] = temp[i]['prob_maps'].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m boxes_batch, scores_batch \u001b[38;5;241m=\u001b[39m \u001b[43mpost_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 51\u001b[0m, in \u001b[0;36mDBPostProcessor.represent\u001b[0;34m(self, batch, _pred)\u001b[0m\n\u001b[1;32m     45\u001b[0m     boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolygons_from_bitmap(\n\u001b[1;32m     46\u001b[0m                             pred[batch_index],\n\u001b[1;32m     47\u001b[0m                             segmentation[batch_index],\n\u001b[1;32m     48\u001b[0m                             inverse_matrix\u001b[38;5;241m=\u001b[39minverse_matrix[batch_index])\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Get boxes from segmentation\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxes_from_bitmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msegmentation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                            \u001b[49m\u001b[43minverse_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minverse_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Append to batch\u001b[39;00m\n\u001b[1;32m     56\u001b[0m boxes_batch\u001b[38;5;241m.\u001b[39mappend(boxes)\n",
      "Cell \u001b[0;32mIn[25], line 162\u001b[0m, in \u001b[0;36mDBPostProcessor.boxes_from_bitmap\u001b[0;34m(self, pred, _bitmap, inverse_matrix)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboxes_from_bitmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred, _bitmap,\n\u001b[1;32m    154\u001b[0m                       inverse_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    Extracts bounding boxes and their scores from a bitmap image.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    _bitmap: single map with shape (1, H, W),\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m        whose values are binarized as {0, 1}\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _bitmap\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    163\u001b[0m     bitmap \u001b[38;5;241m=\u001b[39m _bitmap\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# The first channel\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "boxes_batch, scores_batch = post_processor.represent(data_module.dataset['predict'][0], temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
